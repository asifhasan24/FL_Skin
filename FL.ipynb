{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GjL34665tt8E"
      },
      "outputs": [],
      "source": [
        "from tensorflow import keras\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.layers import Dense, Activation, Flatten, Dropout, BatchNormalization,Embedding\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, ReLU, LSTM,Bidirectional,Attention,Concatenate\n",
        "from tensorflow.keras import regularizers, optimizers,losses\n",
        "from tensorflow.keras.metrics import Recall,Precision,AUC,TruePositives,TrueNegatives,FalseNegatives,FalsePositives, SpecificityAtSensitivity,SensitivityAtSpecificity\n",
        "from tensorflow.keras.utils import plot_model\n",
        "from tensorflow.keras.preprocessing import image\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.python.keras.utils import np_utils\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib\n",
        "import seaborn as sns\n",
        "import sklearn\n",
        "\n",
        "from imblearn.over_sampling import RandomOverSampler\n",
        "import imblearn\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "import os\n",
        "import sklearn.metrics as m\n",
        "from glob import glob\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn import tree\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import train_test_split\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VcTBXTNYtt8H"
      },
      "outputs": [],
      "source": [
        "#classes = {'mel':0 ,'vasc':1,'df':2,'nv':3,'bkl':4,'bcc':5,'ak':6}\n",
        "classes = {4: ('nv',' melanocytic nevi'),\n",
        "           6: ('mel','melanoma'),\n",
        "           2:('bkl','benign keratosis-like lesions'),\n",
        "           1: ('bcc',' basal cell carcinoma'),\n",
        "           5: ('vasc',' pyogenic granulomas and hemorrhage'),\n",
        "           0: ('akiec','Actinic keratoses and intraepithelial carcinomae'),\n",
        "           3: ('df','dermatofibroma')}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6HxK6wYvtt8I"
      },
      "outputs": [],
      "source": [
        "sns.countplot(x = 'dx', data = tabular_data)\n",
        "plt.xlabel('Disease', size=12)\n",
        "plt.ylabel('Frequency', size=16)\n",
        "plt.title('Frequency Distribution of Classes', size=12)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lS8wKkbhtt8I"
      },
      "outputs": [],
      "source": [
        "y = data['label']\n",
        "x = data.drop(columns = ['label'])\n",
        "\n",
        "oversample = RandomOverSampler()\n",
        "x,y  = oversample.fit_resample(x,y)\n",
        "\n",
        "y = to_categorical(y)\n",
        "x= np.array(x).reshape(-1,224,224,3)\n",
        "print('Shape of X :',x.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DVvqFtH3tt8J"
      },
      "outputs": [],
      "source": [
        "x = (x-np.mean(x))/np.std(x)\n",
        "\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(x,y, test_size=0.2, random_state=1)\n",
        "print(X_train.shape , X_test.shape)\n",
        "print(Y_train.shape , Y_test.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vt6mGzH9tt8K"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "IMAGE_SIZE = 224\n",
        "\n",
        "X_train_resized = tf.image.resize(X_train, (IMAGE_SIZE, IMAGE_SIZE))\n",
        "X_test_resized = tf.image.resize(X_test, (IMAGE_SIZE, IMAGE_SIZE))\n",
        "\n",
        "print(X_train_resized.shape, X_test_resized.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jcp-Jaxrtt8L"
      },
      "outputs": [],
      "source": [
        "def blockred(inp, filters):\n",
        "    # Block 1: Inception\n",
        "    x = Conv2D(filters, 1, activation=\"relu\", padding='same', use_bias=False)(inp)\n",
        "    x = layers.MaxPooling2D(2)(x)\n",
        "    x = BatchNormalization(axis=1)(x)\n",
        "\n",
        "    # Block 2: VGG (just VGG with 3x3 convolutions)\n",
        "    y = Conv2D(filters, 3, activation=\"relu\", padding='same', use_bias=False)(inp)\n",
        "    y = Conv2D(filters, 3, activation=\"relu\", padding='same', use_bias=False)(y)\n",
        "    y = layers.MaxPooling2D(2)(y)\n",
        "    y = BatchNormalization(axis=1)(y)\n",
        "\n",
        "    # Block 3: Inception-ResNet\n",
        "    # Parallel 1x1, 3x3, 5x5 convolutions\n",
        "    conv1x1 = Conv2D(filters, 1, activation=\"relu\", padding='same', use_bias=False)(inp)\n",
        "\n",
        "    conv3x3 = Conv2D(filters, 3, activation=\"relu\", padding='same', use_bias=False)(inp)\n",
        "    conv3x3 = Conv2D(filters, 3, activation=\"relu\", padding='same', use_bias=False)(conv3x3)\n",
        "\n",
        "    conv5x5 = Conv2D(filters, 5, activation=\"relu\", padding='same', use_bias=False)(inp)\n",
        "    conv5x5 = Conv2D(filters, 5, activation=\"relu\", padding='same', use_bias=False)(conv5x5)\n",
        "\n",
        "    # Adding the residual connection (skip connection)\n",
        "    # Make sure residual input has the same number of filters\n",
        "    if inp.shape[-1] != filters:\n",
        "        inp = Conv2D(filters, 1, padding='same', use_bias=False)(inp)\n",
        "\n",
        "    inception_resnet_output = layers.add([conv1x1, conv3x3, conv5x5, inp])  # Residual connection added\n",
        "    inception_resnet_output = BatchNormalization(axis=1)(inception_resnet_output)\n",
        "\n",
        "    # MaxPooling and Batch Normalization\n",
        "    inception_resnet_output = layers.MaxPooling2D(2)(inception_resnet_output)\n",
        "\n",
        "    # Ensure that all blocks output the same number of filters\n",
        "    if inception_resnet_output.shape[-1] != filters:\n",
        "        inception_resnet_output = Conv2D(filters, 1, padding='same', use_bias=False)(inception_resnet_output)\n",
        "\n",
        "    # Combine outputs from all blocks\n",
        "    output1 = layers.add([x, y, inception_resnet_output])\n",
        "\n",
        "    return output1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xnyV8Q5Stt8M"
      },
      "outputs": [],
      "source": [
        "Name= \"CNN0\"\n",
        "inputs = keras.Input(shape=(IMAGE_SIZE,IMAGE_SIZE,3), name=\"img\")\n",
        "x = layers.Conv2D(32, 3, activation=\"relu\")(inputs)\n",
        "\n",
        "y = Flatten()(x)\n",
        "y = Dense(256, activation='relu')(y)\n",
        "y = Dropout(0.5)(y)\n",
        "y = Dense(64, activation='relu')(y)\n",
        "y = Dropout(0.5)(y)\n",
        "outputs=Dense(7, activation='softmax')(y)\n",
        "model = keras.Model(inputs, outputs, name=Name)\n",
        "print(model.summary())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fgghpU0Mtt8M"
      },
      "outputs": [],
      "source": [
        "callback = tf.keras.callbacks.ModelCheckpoint(filepath='best_model.keras',\n",
        "                                                  monitor='val_acc', mode='max',\n",
        "                                                 verbose=1)\n",
        "\n",
        "model.compile(optimizer= keras.optimizers.Adam(),\n",
        "              loss=keras.losses.CategoricalCrossentropy() ,\n",
        "              metrics=['acc',Recall(),Precision(),AUC(),TruePositives(),TrueNegatives(),FalseNegatives(),FalsePositives()])\n",
        "\n",
        "#history = model.fit(X_train_resized, Y_train, epochs=5, validation_data=(X_test_resized, Y_test), batch_size=128, callbacks=[callback])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sVBd49entt8N"
      },
      "outputs": [],
      "source": [
        "# Federated learning with homomorphic encryption\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "import tenseal as ts\n",
        "\n",
        "def create_tenseal_context():\n",
        "    poly_modulus_degree = 16384  # Increased polynomial modulus degree\n",
        "    context = ts.context(\n",
        "        ts.SCHEME_TYPE.CKKS,\n",
        "        poly_modulus_degree=poly_modulus_degree,\n",
        "        coeff_mod_bit_sizes=[60, 40, 40, 40, 40, 60]  # More coefficient moduli\n",
        "    )\n",
        "    context.global_scale = 2**40\n",
        "    context.generate_galois_keys()\n",
        "    context.generate_relin_keys()\n",
        "\n",
        "    return context, poly_modulus_degree\n",
        "\n",
        "\n",
        "# Modified encrypt_weights function to handle larger inputs\n",
        "def encrypt_weights(context, weights):\n",
        "    encrypted_weights = []\n",
        "    for layer in weights:\n",
        "        # Flatten and split the weights if they're too large\n",
        "        flat_weights = layer.flatten().tolist()\n",
        "        max_size = poly_modulus_degree // 2 - 1\n",
        "\n",
        "        # Split weights into chunks if needed\n",
        "        if len(flat_weights) > max_size:\n",
        "            chunks = [flat_weights[i:i + max_size] for i in range(0, len(flat_weights), max_size)]\n",
        "            encrypted_chunks = [ts.ckks_vector(context, chunk) for chunk in chunks]\n",
        "            encrypted_weights.append(encrypted_chunks)\n",
        "        else:\n",
        "            encrypted_weights.append(ts.ckks_vector(context, flat_weights))\n",
        "    return encrypted_weights\n",
        "\n",
        "# Modified decrypt_weights function to handle split weights\n",
        "def decrypt_weights(context, encrypted_weights, shapes):\n",
        "    decrypted_weights = []\n",
        "    for enc_layer, shape in zip(encrypted_weights, shapes):\n",
        "        if isinstance(enc_layer, list):  # Handle split weights\n",
        "            decrypted_chunks = []\n",
        "            for chunk in enc_layer:\n",
        "                decrypted_chunks.extend(chunk.decrypt())\n",
        "            decrypted_array = np.array(decrypted_chunks[:np.prod(shape)])\n",
        "        else:\n",
        "            decrypted_array = np.array(enc_layer.decrypt())[:np.prod(shape)]\n",
        "        decrypted_weights.append(decrypted_array.reshape(shape))\n",
        "    return decrypted_weights\n",
        "\n",
        "# Define the number of clients and training rounds\n",
        "NUM_CLIENTS = 5\n",
        "NUM_ROUNDS = 50\n",
        "batch_size = 20\n",
        "epochs = 2\n",
        "\n",
        "# Create encryption context\n",
        "tenseal_context, poly_modulus_degree = create_tenseal_context()\n",
        "\n",
        "# Split the training data among clients\n",
        "clients = []\n",
        "for i in range(NUM_CLIENTS):\n",
        "    client_data = X_train_resized[i * (len(X_train_resized) // NUM_CLIENTS):(i + 1) * (len(X_train_resized) // NUM_CLIENTS)]\n",
        "    client_labels = Y_train[i * (len(Y_train) // NUM_CLIENTS):(i + 1) * (len(Y_train) // NUM_CLIENTS)]\n",
        "    clients.append((client_data, client_labels))\n",
        "\n",
        "# Store accuracy and loss for each federated round\n",
        "client_train_accuracy = [[] for _ in range(NUM_CLIENTS)]\n",
        "client_train_loss = [[] for _ in range(NUM_CLIENTS)]\n",
        "client_val_accuracy = [[] for _ in range(NUM_CLIENTS)]\n",
        "client_val_loss = [[] for _ in range(NUM_CLIENTS)]\n",
        "global_train_accuracy = []\n",
        "global_train_loss = []\n",
        "global_val_accuracy = []\n",
        "global_val_loss = []\n",
        "\n",
        "# Federated learning loop with Homomorphic Encryption\n",
        "for round_num in range(NUM_ROUNDS):\n",
        "    print(f\"\\nüîÑ Federated Round {round_num + 1}/{NUM_ROUNDS}\")\n",
        "\n",
        "    # Select a subset of clients\n",
        "    selected_client_indices = np.random.choice(len(clients), size=int(NUM_CLIENTS * 0.5), replace=False)\n",
        "    selected_clients = [clients[i] for i in selected_client_indices]\n",
        "\n",
        "    # Store encrypted client updates\n",
        "    encrypted_weights_list = []\n",
        "    train_accuracies = []\n",
        "    train_losses = []\n",
        "    val_accuracies = []\n",
        "    val_losses = []\n",
        "\n",
        "    for idx, client in enumerate(selected_clients):\n",
        "        client_model = tf.keras.models.clone_model(model)\n",
        "        client_model.set_weights(model.get_weights())\n",
        "\n",
        "        client_model.compile(optimizer=Adam(learning_rate=0.0001),\n",
        "                             loss='categorical_crossentropy',\n",
        "                             metrics=['accuracy'])\n",
        "\n",
        "        steps_per_epoch = int(len(client[0]) / batch_size)\n",
        "\n",
        "        # Train the client model\n",
        "        history = client_model.fit(client[0], client[1], batch_size=batch_size, epochs=epochs, verbose=1,\n",
        "                                   validation_data=(X_test_resized, Y_test))\n",
        "\n",
        "        # Collect training and validation metrics\n",
        "        train_accuracies.append(history.history['accuracy'][-1])\n",
        "        train_losses.append(history.history['loss'][-1])\n",
        "        val_accuracies.append(history.history['val_accuracy'][-1])\n",
        "        val_losses.append(history.history['val_loss'][-1])\n",
        "\n",
        "        # Store per-client metrics\n",
        "        client_train_accuracy[idx].append(history.history['accuracy'][-1])\n",
        "        client_train_loss[idx].append(history.history['loss'][-1])\n",
        "        client_val_accuracy[idx].append(history.history['val_accuracy'][-1])\n",
        "        client_val_loss[idx].append(history.history['val_loss'][-1])\n",
        "\n",
        "        # Encrypt client weights before sending them to server\n",
        "        encrypted_weights = encrypt_weights(tenseal_context, client_model.get_weights())\n",
        "        encrypted_weights_list.append(encrypted_weights)\n",
        "\n",
        "    # Perform encrypted aggregation (Federated Averaging)\n",
        "    aggregated_encrypted_weights = []\n",
        "    for layer_idx in range(len(encrypted_weights_list[0])):\n",
        "        if isinstance(encrypted_weights_list[0][layer_idx], list):  # Handle split weights\n",
        "            layer_aggregated_chunks = []\n",
        "            for chunk_idx in range(len(encrypted_weights_list[0][layer_idx])):\n",
        "                chunk_aggregated = encrypted_weights_list[0][layer_idx][chunk_idx]\n",
        "                for client_idx in range(1, len(encrypted_weights_list)):\n",
        "                    chunk_aggregated += encrypted_weights_list[client_idx][layer_idx][chunk_idx]\n",
        "                layer_aggregated_chunks.append(chunk_aggregated.mul(1 / len(encrypted_weights_list)))\n",
        "            aggregated_encrypted_weights.append(layer_aggregated_chunks)\n",
        "        else:\n",
        "            layer_aggregated = encrypted_weights_list[0][layer_idx]\n",
        "            for client_idx in range(1, len(encrypted_weights_list)):\n",
        "                layer_aggregated += encrypted_weights_list[client_idx][layer_idx]\n",
        "            aggregated_encrypted_weights.append(layer_aggregated.mul(1 / len(encrypted_weights_list)))\n",
        "\n",
        "    # Decrypt aggregated weights and update global model\n",
        "    model_shapes = [layer.shape for layer in model.get_weights()]\n",
        "    new_weights = decrypt_weights(tenseal_context, aggregated_encrypted_weights, model_shapes)\n",
        "    model.set_weights(new_weights)\n",
        "\n",
        "    # Store round-wise global metrics\n",
        "    global_train_accuracy.append(np.mean(train_accuracies))\n",
        "    global_train_loss.append(np.mean(train_losses))\n",
        "    global_val_accuracy.append(np.mean(val_accuracies))\n",
        "    global_val_loss.append(np.mean(val_losses))\n",
        "\n",
        "# üéØ Final Evaluation\n",
        "y_pred = model.predict(X_test_resized)\n",
        "y_pred_classes = np.argmax(y_pred, axis=1)\n",
        "y_true = np.argmax(Y_test, axis=1)\n",
        "\n",
        "# üèÜ Final Global Model Accuracy\n",
        "from sklearn.metrics import accuracy_score\n",
        "final_accuracy = accuracy_score(y_true, y_pred_classes)\n",
        "print(f\"\\n‚úÖ Final Global Model Accuracy: {final_accuracy * 100:.2f}%\")\n",
        "\n",
        "# üíæ Save Model\n",
        "model.save('Homo_Encrypt_federated_modelc5r50.h5')\n",
        "print(\"\\n‚úÖ Encrypted Federated Model saved successfully!\")\n",
        "\n",
        "# üìä Plot Global Training & Validation Curves\n",
        "fl_rounds = range(1, NUM_ROUNDS + 1)\n",
        "\n",
        "plt.figure(figsize=(20, 5))\n",
        "\n",
        "# Accuracy Plot (Training and Validation)\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(fl_rounds, global_train_accuracy, label=\"Training\", color='blue', marker='o', linewidth=2)\n",
        "plt.plot(fl_rounds, global_val_accuracy, label=\"Validation\", color='green', marker='s', linewidth=2)\n",
        "plt.xlabel(\"Communication Rounds\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.title(\"Global Model Accuracy (with Encrypted Aggregation)\")\n",
        "plt.legend()\n",
        "plt.grid()\n",
        "\n",
        "# Loss Plot (Training and Validation)\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(fl_rounds, global_train_loss, label=\"Training\", color='red', marker='o', linewidth=2)\n",
        "plt.plot(fl_rounds, global_val_loss, label=\"Validation\", color='orange', marker='s', linewidth=2)\n",
        "plt.xlabel(\"Communication Rounds\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.title(\"Global Model Loss (with Encrypted Aggregation)\")\n",
        "plt.legend()\n",
        "plt.grid()\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cuo5uOpJtt8O"
      },
      "outputs": [],
      "source": [
        "res = model.evaluate(X_test_resized, Y_test, verbose=1)\n",
        "print(res)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k3e5vo76tt8O"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Define class names\n",
        "class_names = ['akiec', 'bcc', 'bkl', 'df', 'nv', 'mel', 'vasc']\n",
        "\n",
        "# Predict the values from the test dataset\n",
        "Y_pred = model.predict(X_test_resized)\n",
        "Y_pred_classes = np.argmax(Y_pred, axis=1)\n",
        "Y_true = np.argmax(Y_test, axis=1)\n",
        "\n",
        "# Compute the confusion matrix\n",
        "cm = confusion_matrix(Y_true, Y_pred_classes)\n",
        "\n",
        "# Plot the confusion matrix\n",
        "plt.figure(figsize=(10,7))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=class_names, yticklabels=class_names)\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('True')\n",
        "plt.title('Confusion Matrix')\n",
        "plt.show()\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Asif_tf_gpu",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.16"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}